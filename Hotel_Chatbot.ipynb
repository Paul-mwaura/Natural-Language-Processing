{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hotel Chatbot.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paul-mwaura/Natural-Language-Processing/blob/main/Hotel_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8OGJOkSdQg2"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string # to process standard python strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urj_HSY6gN97"
      },
      "source": [
        "## Reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT6UNszQdSuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2a9bc724-c552-4dd8-9f8d-a82e6655ba05"
      },
      "source": [
        "f=open('Intents.txt','r', errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw=raw.lower()# converts to lowercase\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsdMKEXYgV7F"
      },
      "source": [
        "> **An example of the sent_tokens and the word_tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAmP2kx9dcM7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7428779e-1967-4902-cb5e-2dbdb81a80e3"
      },
      "source": [
        "# Sentence Tokens.\n",
        "#\n",
        "sent_tokens[:6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\"intents\": [\\n        {\"tag\": \"greeting\",\\n         \"patterns\": [\"hi\", \"how are you\", \"is anyone there?',\n",
              " '\", \"hello\", \"good day\"],\\n         \"responses\": [\"hello, thanks for visiting\", \"good to see you again\", \"hi there, how can i help?',\n",
              " '\"],\\n         \"context_set\": \"\"\\n        },\\n        {\"tag\": \"goodbye\",\\n         \"patterns\": [\"bye\", \"see you later\", \"goodbye\"],\\n         \"responses\": [\"see you later, thanks for visiting\", \"have a nice day\", \"bye!',\n",
              " 'come back again soon.\"]',\n",
              " '},\\n        {\"tag\": \"thanks\",\\n         \"patterns\": [\"thanks\", \"thank you\", \"that\\'s helpful\"],\\n         \"responses\": [\"happy to help!',\n",
              " '\", \"any time!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbFWGezgdvV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "109bac6b-41ce-4df5-b068-43ee40585fdf"
      },
      "source": [
        "# Word Tokens.\n",
        "#\n",
        "word_tokens[2:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['intents', \"''\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERsq8MnKgnhA"
      },
      "source": [
        "## Pre-processing the raw text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVFBDph9gntT"
      },
      "source": [
        ">>\n",
        "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-J-TvBHguj9"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdiq1K4cg0MB"
      },
      "source": [
        "## Keyword matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klWOJGEeg0Z3"
      },
      "source": [
        ">>\n",
        "Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmp5n4n3guhp"
      },
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evkZWJnLhWka"
      },
      "source": [
        "## Generating Response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS0soAOghU2I"
      },
      "source": [
        ">>\n",
        "To generate a response from our bot for input questions, the concept of document similarity will be used. So we begin by importing the necessary modules.\n",
        ">>\n",
        "From scikit learn library, import the TFidf vectorizer to convert a collection of raw documents to a matrix of TF-IDF features.\n",
        ">>\n",
        "Also, import cosine similarity module from scikit learn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmjXuXlshRX-"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZH-IZKhpy_"
      },
      "source": [
        "***This will be used to find the similarity between words entered by the user and the words in the corpus. This is the simplest possible implementation of a chatbot.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODTLMz5Thu_b"
      },
      "source": [
        ">>\n",
        "We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31WyOZX4hlY7"
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWRY1Tnbh3n3"
      },
      "source": [
        ">>\n",
        "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon the user’s input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4cellqRh1HN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "97df513e-f730-4b3d-e894-0fe759737dde"
      },
      "source": [
        "flag=True\n",
        "print(\"robo: My name is robo. I will answer your queries about Our hotel. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Robo: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"Robo: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"Robo: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Robo: Bye! take care..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "robo: My name is robo. I will answer your queries about Our hotel. If you want to exit, type Bye!\n",
            "\n",
            "Robo: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I am sorry! I don't understand you\n",
            "Rocket\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Robo: I am sorry! I don't understand you\n",
            "I need help\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Robo: },\n",
            "        {\"tag\": \"thanks\",\n",
            "         \"patterns\": [\"thanks\", \"thank you\", \"that's helpful\"],\n",
            "         \"responses\": [\"happy to help!\n",
            "Bye\n",
            "Robo: Bye! take care..\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}