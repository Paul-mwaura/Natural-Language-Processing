{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter-Pulse-Checker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoZrNiQDlmSDys30B/Bx+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paul-mwaura/Natural-Language-Processing/blob/main/Twitter_Pulse_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62i5ihUX22BB"
      },
      "source": [
        "# Twitter-Pulse-Checker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IzqUGFb2xZL"
      },
      "source": [
        "# import lots of stuff\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn_fT3dw2_5I"
      },
      "source": [
        "# install Flair\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plppFWGz3PiF"
      },
      "source": [
        "# import Flair stuff\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_gCTGtj6xrQ"
      },
      "source": [
        "#import Flair Classifier\n",
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ttNuOJd3VbT"
      },
      "source": [
        "#@title Enter Twitter Credentials\n",
        "TWITTER_KEY = 'HUFlzhnxJbLOqIYaWhXo6VZaz' #@param {type:\"string\"}\n",
        "TWITTER_SECRET_KEY = 'KfiPKZkOpMdI5ony1UXSIYWsICLj9TOkmyQUHqRI488Vs6QWiJ' #@param {type:\"string\"}\n",
        "OAUTH_TOKEN = '1389549456916369408-RnjzLjZZ8YCJo61KA0zOMLT46RZFJq' #@param {type:\"string\"}\n",
        "OAUTH_TOKEN_SECRET = 'HrA0iQsn10eotYmwu5E5iIuXutAH1dOlc67tS3M1OPYES' #@param {type:\"string\"}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o44oFwDf3dp-"
      },
      "source": [
        "# Authenticate\n",
        "auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "    print (\"Can't Authenticate\")\n",
        "    sys.exit(-1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vpTDI8F3tFY"
      },
      "source": [
        "###Lets start scraping!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuXBgFbO3q0C"
      },
      "source": [
        "The Twitter scrape code here was taken from: https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively.\n",
        "\n",
        "My thanks to the author.\n",
        "\n",
        "We need to provide a Search term and a Max Tweet count. Twitter lets you to request 45,000 tweets every 15 minutes  so setting something below that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O__a9b__3wrP",
        "outputId": "c61fbfec-e5b2-4629-a45c-2e699edb70dc"
      },
      "source": [
        "#@title Twitter Search API Inputs\n",
        "#@markdown ### Enter Search Query:\n",
        "searchQuery = 'Kenya' #@param {type:\"string\"}\n",
        "#@markdown ### Enter Max Tweets To Scrape:\n",
        "#@markdown #### The Twitter API Rate Limit (currently) is 45,000 tweets every 15 minutes.\n",
        "maxTweets = 1000 #@param {type:\"slider\", min:0, max:45000, step:100}\n",
        "Filter_Retweets = True #@param {type:\"boolean\"}\n",
        "\n",
        "tweetsPerQry = 100  # this is the max the API permits\n",
        "tweet_lst = []\n",
        "\n",
        "if Filter_Retweets:\n",
        "  searchQuery = searchQuery + ' -filter:retweets'  # to exclude retweets\n",
        "\n",
        "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
        "# else default to no lower limit, go as far back as API allows\n",
        "sinceId = None\n",
        "\n",
        "# If results only below a specific ID are, set max_id to that ID.\n",
        "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
        "max_id = -10000000000\n",
        "\n",
        "tweetCount = 0\n",
        "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
        "while tweetCount < maxTweets:\n",
        "    try:\n",
        "        if (max_id <= 0):\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", since_id=sinceId)\n",
        "        else:\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1))\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1),\n",
        "                                        since_id=sinceId)\n",
        "        if not new_tweets:\n",
        "            print(\"No more tweets found\")\n",
        "            break\n",
        "        for tweet in new_tweets:\n",
        "          if hasattr(tweet, 'reply_count'):\n",
        "            reply_count = tweet.reply_count\n",
        "          else:\n",
        "            reply_count = 0\n",
        "          if hasattr(tweet, 'retweeted'):\n",
        "            retweeted = tweet.retweeted\n",
        "          else:\n",
        "            retweeted = \"NA\"\n",
        "            \n",
        "          # fixup search query to get topic\n",
        "          topic = searchQuery[:searchQuery.find('-')].capitalize().strip()\n",
        "          \n",
        "          # fixup date\n",
        "          tweetDate = tweet.created_at.date()\n",
        "          \n",
        "          tweet_lst.append([tweetDate, topic, \n",
        "                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.text, tweet.favorite_count, \n",
        "                      reply_count, tweet.retweet_count, retweeted])\n",
        "\n",
        "        tweetCount += len(new_tweets)\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "        max_id = new_tweets[-1].id\n",
        "    except tweepy.TweepError as e:\n",
        "        # Just exit if any error\n",
        "        print(\"some error : \" + str(e))\n",
        "        break\n",
        "\n",
        "clear_output()\n",
        "print(\"Downloaded {0} tweets\".format(tweetCount))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded 1000 tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dnWT8KJ38Ue"
      },
      "source": [
        "##Data Sciencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR1byOW36d3"
      },
      "source": [
        "Let's load the tweet data into a Pandas Dataframe so we can do Data Science to it. \n",
        "\n",
        "The data is also saved down in a tweets.csv file in case you want to download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "Zr6Dhpcb33Ru",
        "outputId": "6bd2d6ab-6034-426d-b3f4-d8bd104feab2"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "# load it into a pandas dataframe\n",
        "tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'like_count', 'reply_count', 'retweet_count', 'retweeted'])\n",
        "tweet_df.to_csv('tweets.csv')\n",
        "tweet_df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_dt</th>\n",
              "      <th>topic</th>\n",
              "      <th>id</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>tweet</th>\n",
              "      <th>like_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>retweeted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>Kenya</td>\n",
              "      <td>1396001124306542592</td>\n",
              "      <td>GodMatters2</td>\n",
              "      <td>God Matters</td>\n",
              "      <td>@ITumanka Leviticus 19:28,”You shall not make any cuttings in your flesh for the dead, nor tattoo any marks on you:… https://t.co/Rs346XMNgx</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>Kenya</td>\n",
              "      <td>1396001123312586754</td>\n",
              "      <td>TruthenFacts</td>\n",
              "      <td>Truth&amp; Facts</td>\n",
              "      <td>#UtumishiNaKazi\\nIn July 2020, the National Youth Service delivered over 20,000 new uniforms to the Kenya Police Ser… https://t.co/s2Xj6yTAPL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>Kenya</td>\n",
              "      <td>1396001109471285248</td>\n",
              "      <td>MwasyaKyalo</td>\n",
              "      <td>Mwasya Kyalo</td>\n",
              "      <td>In terms of benefits, the project, he said, is set to benefit the 11-million litre Nanyuki Petroleum depot operated… https://t.co/f2M0loVipu</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>Kenya</td>\n",
              "      <td>1396001108150079495</td>\n",
              "      <td>trilmindtweets</td>\n",
              "      <td>MataraOmwenga</td>\n",
              "      <td>The die is cast. A very powerful legal team with great legal minds. The task ahead though is Herculean. The ruling… https://t.co/EjsQ1GTBuL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>Kenya</td>\n",
              "      <td>1396001059550736388</td>\n",
              "      <td>high__witness</td>\n",
              "      <td>Mukhisa Kituyi for President 2022</td>\n",
              "      <td>Today I celebrate The Bee Oueens of Nasaru Olosho Nature Conservancy; a joint project of the @Environment_Ke and… https://t.co/etCqPVz5eY</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_dt  topic  ...  retweet_count retweeted\n",
              "0  2021-05-22  Kenya  ...  0              False   \n",
              "1  2021-05-22  Kenya  ...  0              False   \n",
              "2  2021-05-22  Kenya  ...  0              False   \n",
              "3  2021-05-22  Kenya  ...  0              False   \n",
              "4  2021-05-22  Kenya  ...  0              False   \n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8vBQLaR5ERC"
      },
      "source": [
        "Unfortunately Twitter does not let you filter by date when you request tweets. However, we can do this at this stage. I have set it up to pull yesterday + todays Tweets by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cn0UuRQ4AqG",
        "outputId": "9fca0229-8182-456d-e374-b09c164a751d"
      },
      "source": [
        "#@title Filter By Date Range\n",
        "today = datetime.now().date()\n",
        "yesterday = today - timedelta(1)\n",
        "\n",
        "start_dt = '' #@param {type:\"date\"}\n",
        "end_dt = '' #@param {type:\"date\"}\n",
        "\n",
        "if start_dt == '':\n",
        "  start_dt = yesterday\n",
        "else:\n",
        "  start_dt = datetime.strptime(start_dt, '%Y-%m-%d').date()\n",
        "\n",
        "if end_dt == '':\n",
        "  end_dt = today\n",
        "else:\n",
        "  end_dt = datetime.strptime(end_dt, '%Y-%m-%d').date()\n",
        "\n",
        "\n",
        "tweet_df = tweet_df[(tweet_df['tweet_dt'] >= start_dt) \n",
        "                    & (tweet_df['tweet_dt'] <= end_dt)]\n",
        "tweet_df.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cohXrO5Mu2"
      },
      "source": [
        "## NER and Sentiment Analysis\n",
        "\n",
        "Now let's do some NER / Sentiment Analysis. We will use the Flair library: https://github.com/zalandoresearch/flair\n",
        "\n",
        "###NER\n",
        "\n",
        "Previosuly, we extracted, and then appended the Tags as separate rows in our dataframe. This helps us later on to Group by Tags.\n",
        "\n",
        "We also create a new 'Hashtag' Tag as Flair does not recognize it and it's a big one in this context.\n",
        "\n",
        "### Sentiment Analysis\n",
        "\n",
        "We use the Flair Classifier to get Polarity and Result and add those fields to our dataframe.\n",
        "\n",
        "**Warning:** This can be slow if you have lots of tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYpOidBr5Hzi",
        "outputId": "6a4c2393-cc10-4b93-fa75-165a1ec733b1"
      },
      "source": [
        "# predict NER\n",
        "nerlst = []\n",
        "\n",
        "for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):\n",
        "  cleanedTweet = row['tweet'].replace(\"#\", \"\")\n",
        "  sentence = Sentence(cleanedTweet, use_tokenizer=True)\n",
        "  \n",
        "  # predict NER tags\n",
        "  tagger.predict(sentence)\n",
        "\n",
        "  # get ner\n",
        "  ners = sentence.to_dict(tag_type='ner')['entities']\n",
        "  \n",
        "  # predict sentiment\n",
        "  classifier.predict(sentence)\n",
        "  \n",
        "  label = sentence.labels[0]\n",
        "  response = {'result': label.value, 'polarity':label.score}\n",
        "  \n",
        "  # get hashtags\n",
        "  hashtags = re.findall(r'#\\w+', row['tweet'])\n",
        "  if len(hashtags) >= 1:\n",
        "    for hashtag in hashtags:\n",
        "      ners.append({ 'type': 'Hashtag', 'text': hashtag })\n",
        "  \n",
        "  for ner in ners:\n",
        "    adj_polarity = response['polarity']\n",
        "    if response['result'] == 'NEGATIVE':\n",
        "      adj_polarity = response['polarity'] * -1\n",
        "    try:\n",
        "      ner['type']\n",
        "    except:\n",
        "      ner['type'] = ''      \n",
        "    nerlst.append([ row['tweet_dt'], row['topic'], row['id'], row['username'], \n",
        "                   row['name'], row['tweet'], ner['type'], ner['text'], response['result'], \n",
        "                   response['polarity'], adj_polarity, row['like_count'], row['reply_count'], \n",
        "                  row['retweet_count'] ])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 803/1000 [28:44<05:50,  1.78s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8FBifwK5Pd1"
      },
      "source": [
        "df_ner = pd.DataFrame(nerlst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'tag_type', 'tag', 'sentiment', 'polarity', \n",
        "                                       'adj_polarity','like_count', 'reply_count', 'retweet_count'])\n",
        "df_ner.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6qGWQ07I2L"
      },
      "source": [
        "Let's filter out obvious tags like #Seattle that would show up for this search. You can comment this portion out or use different Tags for your list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCQjwY-p7EDM"
      },
      "source": [
        "# filter out obvious tags\n",
        "banned_words = ['Kenya', 'Ke', '#Nairobi', '#Mombasa', 'Kisumu']\n",
        "\n",
        "df_ner = df_ner[~df_ner['tag'].isin(banned_words)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1XpL18D7wb3"
      },
      "source": [
        "Calculate Frequency, Likes, Replies, Retweets and Average Polarity per Tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXWkjWKd7bY5"
      },
      "source": [
        "ner_groups = df_ner.groupby(['tag', 'tag_type']).agg({'tag': \"count\", 'adj_polarity': \"mean\",\n",
        "                                                     'like_count': 'sum', 'reply_count': 'sum',\n",
        "                                                     'retweet_count': 'sum'})\n",
        "ner_groups = ner_groups.rename(columns={\n",
        "    \"tag\": \"Frequency\",\n",
        "    \"adj_polarity\": \"Avg_Polarity\",\n",
        "    \"like_count\": \"Total_Likes\",\n",
        "    \"reply_count\": \"Total_Replies\",\n",
        "    \"retweet_count\": \"Total_Retweets\"\n",
        "})\n",
        "ner_groups = ner_groups.sort_values(['Frequency'], ascending=False)\n",
        "ner_groups = ner_groups.reset_index()\n",
        "ner_groups.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKgN1SA_76MB"
      },
      "source": [
        "Create an overall Sentiment column based on the Average Polarity of the Tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udqITE_z705q"
      },
      "source": [
        "ner_groups['Sentiment'] = np.where(ner_groups['Avg_Polarity']>=0, 'POSITIVE', 'NEGATIVE')\n",
        "ner_groups.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0pISoGZ8Bs1"
      },
      "source": [
        "## Visualize!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MdRXenF8GlA"
      },
      "source": [
        "We can get some bar plots for the Tags based on the following metrics:\n",
        "\n",
        "*   Most Popular Tweets\n",
        "*   Most Liked Tweets\n",
        "*   Most Replied Tweets\n",
        "*   Most Retweeted Tweets\n",
        "\n",
        "By default, we do the analysis on all the Tags but we can also filter by Tag by checking the Filter_TAG box. \n",
        "This way we can further drill down into the metrics for Hashtags, Persons, Locations & Organizations.\n",
        "\n",
        "We cut the plots by Sentiment i.e. the color of the bars tells us if the overall Sentiment was Positive or Negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTT1wAaK79qo"
      },
      "source": [
        "#@title Visualize Top TAGs\n",
        "Filter_TAG = False #@param {type:\"boolean\"}\n",
        "TAG = 'Person' #@param [\"Hashtag\", \"Person\", \"Location\", \"Organization\"]\n",
        "#@markdown ###Pick how many tags to display per chart:\n",
        "Top_N = 10 #@param {type:\"integer\"}\n",
        "\n",
        "# get TAG value\n",
        "if TAG != 'Hashtag':\n",
        "  TAG = TAG[:3].upper()\n",
        "\n",
        "if Filter_TAG:\n",
        "  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]\n",
        "else:\n",
        "  filtered_group = ner_groups\n",
        "\n",
        "# plot the figures\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.5)\n",
        "\n",
        "ax1 = fig.add_subplot(321)\n",
        "sns.barplot(x=\"Frequency\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax2 = fig.add_subplot(322)\n",
        "filtered_group = filtered_group.sort_values(['Total_Likes'], ascending=False)\n",
        "sns.barplot(x=\"Total_Likes\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax3 = fig.add_subplot(323)\n",
        "filtered_group = filtered_group.sort_values(['Total_Replies'], ascending=False)\n",
        "sns.barplot(x=\"Total_Replies\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax4 = fig.add_subplot(324)\n",
        "filtered_group = filtered_group.sort_values(['Total_Retweets'], ascending=False)\n",
        "sns.barplot(x=\"Total_Retweets\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "\n",
        "ax1.title.set_text('Most Popular')\n",
        "ax2.title.set_text('Most Liked')\n",
        "ax3.title.set_text('Most Replied')\n",
        "ax4.title.set_text('Most Retweeted')\n",
        "\n",
        "ax1.set_ylabel('')    \n",
        "ax1.set_xlabel('')\n",
        "ax2.set_ylabel('')    \n",
        "ax2.set_xlabel('')\n",
        "ax3.set_ylabel('')    \n",
        "ax3.set_xlabel('')\n",
        "ax4.set_ylabel('')    \n",
        "ax4.set_xlabel('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oms70R8V8TIu"
      },
      "source": [
        "###Get the Average Polarity Distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIU4Y16K8OtH"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.distplot(filtered_group['Avg_Polarity'], hist=False, kde_kws={\"shade\": True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEF6K9eV8aGm"
      },
      "source": [
        "## Word Cloud\n",
        "\n",
        "Let's build a Word Cloud based on these metrics. \n",
        "\n",
        "Since I am interested in Seattle, I am going to use overlay the Seattle city skyline view over my Word Cloud. \n",
        "You can change this by selecting a different Mask option from the drop down.\n",
        "\n",
        "Images for Masks can be found at:\n",
        "\n",
        "http://clipart-library.com/clipart/2099977.htm\n",
        "\n",
        "https://needpix.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p4UlA-a8WQv"
      },
      "source": [
        "# download mask images\n",
        "!wget http://clipart-library.com/img/2099977.jpg -O seattle.jpg\n",
        "!wget https://storage.needpix.com/rsynced_images/trotting-horse-silhouette.jpg -O horse.jpg\n",
        "!wget https://storage.needpix.com/rsynced_images/black-balloon.jpg -O balloon.jpg\n",
        "  \n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf5buTrm9Byg"
      },
      "source": [
        "#@title Build Word Cloud For Top TAGs\n",
        "Metric = 'Most Popular' #@param [\"Most Popular\", \"Most Liked\", \"Most Replied\", \"Most Retweeted\"]\n",
        "#@markdown\n",
        "Filter_TAG = False #@param {type:\"boolean\"}\n",
        "##@markdown\n",
        "TAG = 'Location' #@param [\"Hashtag\", \"Person\", \"Location\", \"Organization\"]\n",
        "Mask = 'Seattle' #@param [\"Rectangle\", \"Seattle\", \"Balloon\", \"Horse\"]\n",
        "\n",
        "# get correct Metric value\n",
        "if Metric == 'Most Popular':\n",
        "   Metric = 'Frequency'\n",
        "elif Metric == 'Most Liked':\n",
        "   Metric = 'Total_Likes'\n",
        "elif Metric == 'Most Replied':\n",
        "   Metric = 'Total_Replies'\n",
        "elif Metric == 'Most Retweeted':\n",
        "   Metric = 'Total_Retweets'    \n",
        "\n",
        "# get TAG value\n",
        "if TAG != 'Hashtag':\n",
        "  TAG = TAG[:3].upper()\n",
        "\n",
        "if Filter_TAG:\n",
        "  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]\n",
        "else:\n",
        "  filtered_group = ner_groups\n",
        "\n",
        "countDict = {}\n",
        "\n",
        "for index, row in filtered_group.iterrows():\n",
        "  if row[Metric] == 0:\n",
        "    row[Metric] = 1\n",
        "  countDict.update( {row['tag'] : row[Metric]} )\n",
        "  \n",
        "if Mask == 'Seattle':\n",
        "  Mask = np.array(Image.open(\"seattle.jpg\"))\n",
        "elif Mask == 'Rectangle':\n",
        "  Mask = np.array(Image.new('RGB', (800,600), (0, 0, 0)))\n",
        "elif Mask == 'Horse':\n",
        "  Mask = np.array(Image.open(\"horse.png\"))\n",
        "elif Mask == 'Balloon':\n",
        "  Mask = np.array(Image.open(\"balloon.jpg\"))\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Generate Word Cloud\n",
        "wordcloud = WordCloud(\n",
        "    max_words=100,\n",
        "#     max_font_size=50,\n",
        "    height=300,\n",
        "    width=800,\n",
        "    background_color = 'white',\n",
        "    mask=Mask,\n",
        "    contour_width=1,\n",
        "    contour_color='steelblue',\n",
        "    stopwords = STOPWORDS).generate_from_frequencies(countDict)\n",
        "fig = plt.figure(\n",
        "    figsize = (18, 18),\n",
        "    )\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxQfVPRY9En3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}